  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiCVf2fFd2k-"
   },
   "source": [
    "# Model Evaluation\n",
    "\n",
    "\n",
    "This code loads the pre-trained Faster RCNN object detection model, defines the COCO instance category names, and provides a helper function for converting base64-encoded image data to a NumPy array using OpenCV. Additionally, it prompts the user to upload a video file and performs object detection on each frame of the video, displaying the frames with bounding boxes and labels for detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\chuksonwubolu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (9.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoCox4yobkp5",
    "outputId": "441f5b5d-9742-4d39-d111-5f084e84e8f8"
   },
   "outputs": [],
   "source": [
    "# @title Download or load model if True or False respectively\n",
    "# Define the computation device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.COCO_V1', weights_backbone=None).to(device)\n",
    "# Set the model to evaluation mode\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WlTzQ16GVgef"
   },
   "outputs": [],
   "source": [
    "# @title COCO INSTANCE CATEGORY NAMES\n",
    "COCO_INSTANCE_CATEGORY_NAMES =  [\n",
    "    '__background__',\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "    'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aCUYIZB0Sel0"
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "def js_to_image(js_reply):\n",
    "    '''\n",
    "    Functionality: Convert a base64-encoded image received as JavaScript reply to a NumPy array image using OpenCV.\n",
    "\n",
    "    Parameters:\n",
    "    - js_reply (str): JavaScript reply containing base64-encoded image data.\n",
    "\n",
    "    Returns:\n",
    "    - image (numpy.ndarray): NumPy array representing the decoded image.\n",
    "    '''\n",
    "    # Split the JavaScript reply to extract the base64-encoded image data\n",
    "    image_data = base64.b64decode(js_reply.split(',')[1])\n",
    "\n",
    "    # Convert the decoded image data to a NumPy array of uint8\n",
    "    image = np.frombuffer(image_data, np.uint8)\n",
    "\n",
    "    # Decode the NumPy array as an image using OpenCV's imdecode function\n",
    "    # The flag cv2.IMREAD_COLOR specifies to read the image in color\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Return the resulting image\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-hfDS-h0LLdZ",
    "outputId": "2ef96ebf-37d7-40e1-97d4-e0c598e3fc35"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Functionality: This script performs object detection on a video file using a pre-trained model.\n",
    "\n",
    "Variables:\n",
    "- video_path: Path to the input video file.\n",
    "- cap: VideoCapture object for reading the video frames.\n",
    "- fps: Frames per second of the input video.\n",
    "- width: Width of each frame in pixels.\n",
    "- height: Height of each frame in pixels.\n",
    "- output_path: Path to the output video file.\n",
    "- fourcc: Codec used for the output video.\n",
    "- out: VideoWriter object for writing the processed frames to the output video.\n",
    "- frame_pil: PIL Image object representing the current video frame.\n",
    "- transform: torchvision.transforms object for converting the PIL Image to a PyTorch tensor.\n",
    "- image: PyTorch tensor representing the current video frame.\n",
    "- pred: Prediction output from the object detection model.\n",
    "- pred_class: List of predicted classes for detected objects in the frame.\n",
    "- pred_boxes: List of bounding boxes for detected objects in the frame.\n",
    "- pred_score: List of confidence scores for the detected objects.\n",
    "- pred_t: Index of the last object detection result above the confidence threshold.\n",
    "'''\n",
    "\n",
    "\n",
    "# Specify the path to your video file\n",
    "video_path = 'road_trafifc.mp4'\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "# Specify the output video file\n",
    "output_path = 'output.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to a PIL image for prediction\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Convert the image to a PyTorch tensor\n",
    "    transform = T.ToTensor()\n",
    "    image = transform(frame_pil)\n",
    "\n",
    "    # Add a batch dimension to the image\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(image)\n",
    "        pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "        pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "\n",
    "    pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "    pred_t = -1  # Initialize pred_t to -1\n",
    "\n",
    "    # Check if there are elements in pred_score greater than 0.7\n",
    "    if any(x > 0.7 for x in pred_score):\n",
    "        pred_t = [pred_score.index(x) for x in pred_score if x > 0.7][-1]\n",
    "\n",
    "    if pred_t >= 0:\n",
    "        pred_boxes = pred_boxes[:pred_t + 1]\n",
    "        pred_class = pred_class[:pred_t + 1]\n",
    "\n",
    "    for i in range(len(pred_boxes)):\n",
    "        cv2.rectangle(frame, tuple(map(int, pred_boxes[i][0])), tuple(map(int, pred_boxes[i][1])), color=(0, 255, 0),\n",
    "                      thickness=2)\n",
    "        cv2.putText(frame, pred_class[i], tuple(map(int, pred_boxes[i][0])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0),\n",
    "                    thickness=1)\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    # cv2_imshow(frame)\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the OpenCV window\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
